# LLM-compression-quantization-distillation
Efficient optimization techniques for large language models: quantization, pruning, and distillation.
# ğŸš€ Optimization Techniques for Large Language Models (LLMs)

This repository presents a comprehensive study and implementation overview of optimization strategies used to make large language models more efficient in terms of memory, latency, and cost â€” without significantly sacrificing performance.

## ğŸ§  Why Optimize LLMs?

Large Language Models (LLMs) such as GPT-4 and LLaMA-65B are powerful but computationally expensive. Optimization is essential for:
- Reducing inference cost and latency
- Enabling deployment on limited hardware
- Improving energy efficiency and sustainability

## ğŸ”§ Techniques Covered

### 1. Quantization
- Post-Training Quantization (PTQ)
- Quantization-Aware Training (QAT)
- INT8, NF4, and Double Quantization (DQ)
- Implementation examples using QLoRA

### 2. Pruning
- Unstructured pruning (magnitude-based)
- Structured pruning (attention heads, FFN layers)
- Activation-aware pruning
- Discussion on sparsity and hardware constraints

### 3. Distillation
- Knowledge distillation pipeline
- Self-Instruct with synthetic data generation
- Student-teacher training loop with soft/hard targets

### 4. Mixture of Experts (MoE) *(planned section)*

## ğŸ“Š Practical Examples

- Performance analysis: FP16 vs INT8
- Case study: GPT-4 vs GPT-4 Mini
- Memory & speed benchmarks using NVIDIA H100

## ğŸ§ª Tools & Frameworks

- PyTorch
- Huggingface Transformers
- QLoRA
- NumPy, Matplotlib
- Optuna (for tuning)

## ğŸ” Research Impact

> Efficient LLM optimization is a critical step toward democratizing access to powerful AI models and ensuring their sustainability in real-world systems.

## ğŸ“š References

- â€œLLM.int8(): 8-bit Matrix Multiplication for Transformersâ€
- â€œQLoRA: Efficient Finetuning of Quantized LLMsâ€
- â€œDistilBERT: Smaller, faster, cheaper and lighterâ€

## ğŸ‘¤ Author

**[Fateme Taroodi]**  
M.Sc. in Applied Mathematics â€“ Data Science  
Shahid Beheshti University, Tehran, Iran  
Researcher in LLM Optimization, PINNs, and Reinforcement Learning  
